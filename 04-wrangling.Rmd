# Data Wrangling

## Aims

- Data organisation principles, tidy data
- Introduction to the [Tidyverse](https://www.tidyverse.org/) syntax in R.
- Efficiently working with large environmental datasets and combining.

## Contents

- Variables in a dataframe: select, rename
- Time: lubridate ([link](https://datacarpentry.org/R-ecology-lesson/02-starting-with-data.html#Formatting_Dates))
- Variable (re-) definition: mutate
- Aggregating: group_by, summarise, count, arrange
- Cleaning (outliers, quality flags, ...) and gapfilling: filter
- Combining: join_
- Workflows: piping
- purrr
- purrr and mutate
- Advanced data vis
- Tidy data
- Wide and long table formats: pivont_longer, ...
- Factors, dimensions
- Visualise ([link](https://datacarpentry.org/R-ecology-lesson/04-visualization-ggplot2.html))
  - factors, boxplot
  - mapping aesthetics: color, shape
- Flat and nested data frames: tidyr

## Prerequisites

- Knowing how to read data into R
- Access datasets 1 and 2
- Possible to require additional dataset: SwissFACE (experimental data!)


## Exercises

- **Input**: The students 
- **Output**: The student will produce a plant functional trait maps for Europe that can be considered later as a predictor. They have c about Biome classification, IGBP veg types
- **Data**: Dataset 2 in particular the geographic locations for which vegetation types will be produced in combination with plant occurrences (GBIF) and plant trait data (leaf trait - TRY).

## Tutorial

### Dataset 1 (HH flux data)

#### Variables in a dataframe

- select
  - starts_with, ends_with, contains, matches, num_range
- rename

[link](https://r4ds.had.co.nz/transform.html#select)

Let's read in the half-hourly data from the eddy-covariance site CH-Lae again (as we did in Chapter 1) and have a quick look at it. We use the function `read_csv()` from the readr package here for reading the CSV since it's faster than the base-R `read.csv()` and generates a nicely readable output when printing the object. More info about base R and tidyverse data reading [here](https://r4ds.had.co.nz/data-import.html).
```{r}
library(readr)
hhdf <- read_csv("data_HH/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2014_1-3.csv")
hhdf
```
This was easy. The file is automatically machine-readable without us having to specify additional information, because

- Only one header row, containing the column (variable) names
- No annoying white spaces in column names
- No merged cells or alike
- ...

<!-- But some things are not as intended. For example, some cells contain the number '-9999'. From the data description, we know that this is the code for a missing data point. R offers the `NA` object for missing values. If `read_csv()` does not identify these values automatically, the code must be provided: -->
<!-- ```{r} -->
<!-- hhdf <- read_csv("data_HH/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2014_1-3.csv", na = "-9999") -->
<!-- hhdf -->
<!-- ``` -->

Print dimensions
```{r}
dim(hhdf)
```

Column names, only first few
```{r}
names(hhdf)[1:30]
```

This dataset is "tidy", meaning that variables are organised by columns, and individual observations (here different points in time) are given by rows.

Select variables.
Base R by column index (number)
```{r}
hhdf[,1:4]
```
Base R by column names
```{r}
hhdf[,c("TIMESTAMP_START", "TIMESTAMP_END", "TA_F_MDS", "TA_F_MDS_QC")]
```

Tidyverse. (introduce it briefly)
```{r}
library(dplyr)
select(hhdf, TIMESTAMP_START, TIMESTAMP_END, TA_F_MDS, TA_F_MDS_QC)
```

It's good to reduce the size of objects to make processing faster. Below, we're going to use only a small subset of the 235 variables. In particular, we're interested in GPP (gross primary production) and PPFD (photosynthetic photon flux density). Our dataset includes multiple columns of GPP derived from eddy covariance measurements using different methods. Respective column names all start with `"GPP_"`... One method to derive GPP uses a variable u-star filtering , the other method uses a constant u-star filtering (don't mind what it actually means). Respective column names contain the strings `"VUT"`, and `"CUT"`, respectively. Say, we are only interested in the GPP derived based on the former. Hence, we can drop all columns that contain `"CUT"` in their names. We also want to keep columns that start with `"TIMESTAMP_"` and all variables starting with "`NEE_`" and ending with `"_QC"` ("net ecosystem exchange quality control", used for filtering further below).
Let's apply this selection for all further processing of the `hhdf` dataframe.
```{r}
select(hhdf,
       starts_with("TIMESTAMP_"), 
       starts_with("GPP_"),
       starts_with("PPFD_"),
       starts_with("NEE_") & ends_with("_QC"),
       -contains("CUT")
       )
```

Note that the selection criteria are evaluated in the order we write them in the `select()` function call. 

Complete reference for selecting variables is [here](https://dplyr.tidyverse.org/reference/select.html).

#### Time

- lubridate ([link](https://datacarpentry.org/R-ecology-lesson/02-starting-with-data.html#Formatting_Dates))

[link](https://r4ds.had.co.nz/dates-and-times.html)

Weird interpretation of the variables `"TIMESTAMP_START"` and `"TIMESTAMP_END"`.
```{r}
typeof(hhdf$TIMESTAMP_START[[1]])
as.character(hhdf$TIMESTAMP_START[[1]])
```

Format is: YYYYMMDDhhmm. Use lubridate package to correctly interpret. We're modifying a variable. 
```{r}
library(lubridate)
dates <- ymd_hm(hhdf$TIMESTAMP_START)
head(dates)
```

Easy work with dates now
```{r}
nextday <- dates + days(1)
head(nextday)
```

```{r}
month_of_year <- month(dates)
head(month_of_year)
```

etc.


#### Variable (re-) definition

- mutate
- pipes (`%>%`)

[link](https://r4ds.had.co.nz/transform.html#add-new-variables-with-mutate)

```{r}
mutate(hhdf, TIMESTAMP_START = ymd_hm(TIMESTAMP_START), TIMESTAMP_END = ymd_hm(TIMESTAMP_END))
```

We've already done several two typical steps of a data science workflow: Selecting variables, and modifying variables. The tidyverse offers an intuitive syntax for writing code that is particularly useful to implement workflows....

(introduce the magrittr pipe `%>%`)
```{r}
hhdf <- hhdf %>% 
  select(starts_with("TIMESTAMP_"), starts_with("GPP_"), starts_with("PPFD_"), starts_with("NEE_") & ends_with("_QC"), -contains("CUT")) %>% 
  mutate(TIMESTAMP_START = ymd_hm(TIMESTAMP_START), TIMESTAMP_END = ymd_hm(TIMESTAMP_END))
```

#### Cleaning (outliers, quality flags, ...) and gapfilling

- filter
- missing values

[link](https://r4ds.had.co.nz/transform.html#filter-rows-with-filter)

Data has quality flags (`"_QC"`). Get good-quality NEE data. Codes are:

0 = measured
1 = good quality gap-filled
2 = medium
3 = poor

Let's take only actually measured or good quality gap-filled data.
```{r eval=FALSE}
hhdf %>% 
  filter(NEE_VUT_REF_QC == 0 | NEE_VUT_REF_QC == 1)
```

This can be written more simply with the `%in%` (... "is element of" ...) as:
```{r eval=FALSE}
hhdf %>% 
  filter(NEE_VUT_REF_QC %in% c(0,1))
```

This removes rows (note the information about number of rows printed). In some cases this is undesired. We can also replace bad-quality NEE values with NA.
```{r}
hhdf %>% 
  mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC %in% c(0,1), GPP_NT_VUT_REF, NA))
```

Some values are -9999. When reading the documentation of the dataset, we learn that this is the code for missing data. Replace such values in any column (except the columns starting with `"TIMESTAMP_"`) with NA and implement our mutations for good.
```{r}
replace_na <- function(x){ifelse(x == -9999, NA, x)}

hhdf <- hhdf %>% 
  mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC %in% c(0,1), GPP_NT_VUT_REF, NA)) %>% 
  mutate_at(vars(-starts_with("TIMESTAMP_")), replace_na)
```

#### Visualisation

- simple x-y plot

Looking at data is an integral part of data processing. Here we introduce just the very basics. Refer to other resources.

Let's look at the first 1440 time steps (corresponding to 30 days)
```{r}
plot(hhdf$TIMESTAMP_START, hhdf$GPP_NT_VUT_REF, type = "l")
```

Introduce ggplot briefly

```{r}
library(ggplot2)
ggplot(data = hhdf, aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) +
  geom_line()
```

#### Arranging, etc. 

- head, tail
- arrange
  - desc()
- distinct
- count

Look at the data frame directly


#### Aggregating

- group_by
- summarise
  - mean, median, sd, quantile
  - first, last
  - n_distinct
  
[link](https://r4ds.had.co.nz/transform.html#grouped-summaries-with-summarise)

```{r}
ddf <- hhdf %>% 
  mutate(date = as_date(TIMESTAMP_START)) %>% 
  group_by(date) %>% 
  summarise(GPP_NT_VUT_REF = sum(GPP_NT_VUT_REF, na.rm = TRUE), .groups = 'drop')
```

Plot all days in year 2007.
```{r}
ddf %>% 
  filter(year(date)==2007) %>%  # same functions as above can be applied to 'date'
  ggplot(aes(date, GPP_NT_VUT_REF)) +
  geom_line()
  # xlim(ymd("2007-01-01"), ymd("2007-12-31")) # alternative to reducing x axis of plot (not of data)
```

#### Workflow example

- combining multiple processing steps (filter, summarise) into a function
- data vis II
  - ~color
  - overplotting density plot
  - smoothing line
  - factor
- linear regression `lm`
- define function

xxxxx
```{r}
proc_lightresponse <- function(df){
  
  df <- df %>% 
    
    mutate(date = as_date(TIMESTAMP_START)) %>% 

    # aggregate to daily totals
    group_by(date) %>% 
    summarise(GPP_NT_VUT_REF = sum(GPP_NT_VUT_REF),
              PPFD_IN = sum(PPFD_IN),
              n_datapoints = n(),
              n_ppfd = n(),
              n_measured = sum(NEE_VUT_REF_QC == 0),
              n_goodquality = sum(NEE_VUT_REF_QC %in% c(0,1)),
              .groups = 'drop'
              ) %>% 
    mutate(f_measured = n_measured / n_datapoints,
           f_goodquality = n_goodquality / n_datapoints)

  return(df)
}  

df <- hhdf %>%  
  proc_lightresponse()
```

```{r}
df %>%
  ggplot(aes(x = PPFD_IN, y = GPP_NT_VUT_REF)) +
  geom_point()
```

```{r}
df %>%
  ggplot(aes(x = PPFD_IN, y = GPP_NT_VUT_REF, color = f_measured)) +
  geom_point() +
  scale_color_viridis_c(direction = -1)
```


```{r}
df %>%
  ggplot(aes(x = PPFD_IN, y = GPP_NT_VUT_REF)) +
  stat_density_2d(
    geom = "raster",
    aes(fill = after_stat(density)),
    contour = FALSE
    ) +
  scale_fill_viridis_c()
```

```{r}
linmod <- lm(GPP_NT_VUT_REF ~ PPFD_IN, data = df)
```


```{r}
df %>%
  ggplot(aes(x = PPFD_IN, y = GPP_NT_VUT_REF)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red")
```

Different relationship whether f_measured is greater 0.5:
```{r}
df %>%
  mutate(more_measured = as.factor(f_measured > 0.5)) %>% 
  ggplot(aes(x = PPFD_IN, y = GPP_NT_VUT_REF, color = more_measured)) +
  geom_point() +
  geom_smooth(method = "lm") 
```
Doesn't look like it.

```{r}
df %>%
  mutate(month = as.factor(month(date))) %>% 
  ggplot(aes(x = PPFD_IN, y = GPP_NT_VUT_REF, color = month)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE) 
```

We already found a pattern. Actually two: More light -> more GPP. Relationship (sensitivity) depends on the season (here: month).

### Dataset 2 (DD flux data)

#### Frunctional programming

- purrr

Find all daily time series data in the `"./data_DD"` directory. Files are identified here by their name, which contains the pattern `"DD"` (for 'daily').
```{r}
vec_files <- list.files("./data_DD", pattern = "_FLUXNET2015_FULLSET_DD_", full.names = TRUE)
```

This returns 36 files for 36 sites. We can read them in at once using a simple loop. Here, we are creating a list of data frames of length 36.
```{r message=FALSE, eval=FALSE}
library(readr)
list_df <- list()
for (ifil in vec_files){
  list_df[[ifil]] <- read_csv(ifil)
}
```

Here is a quick deviation into functional programming. Note that a loop basically consists of two "components": a counter index (here `ifil`), and some statement that is applied at each iteration where the counter index takes a new value. This concept can be generalised further by understanding the counter index as an element of a list and the statement as any function that is "mapped" onto the list of elements. We have arrived at functional programming. The purrr package offers the tools for functional programming in R.

The above loop can be written on one line as:
```{r message=FALSE}
library(purrr)
list_df <- map(as.list(vec_files), ~read_csv(., na = "-9999"))
names(list_df) <- vec_files
```

Note NA value specified.

It may be unpractical to have the different dataframes as elements of a list. In fact, the data frames read in here each have similar shapes. I.e., they share the same columns (but differ by their number of rows, and of course, by their data values). This suggests that we can "stack" each dataframes along rows.
```{r}
df_allsites <- bind_rows(list_df, .id = "siteid")
df_allsites
```

The column `TIMESTAMP` ...
```{r}
df_allsites <- df_allsites %>% 
  mutate(TIMESTAMP = ymd(TIMESTAMP))
```

#### Strings

- stringr

This creates one single data frame containing all sites' data (>90'000 rows), and adds a column named `"siteid"` that is automatically created by using the names of the list elements of `list_df`. Unfortunately, this contains strings specifying the full paths of the files that were read. We would like to extract the site name from these strings. Fortunately, the file names follow a clear pattern (see: naming your files wisely is more important than you would think at first!). 
```{r}
vec_files %>% head()
```

The paths each start with the subdirectory where they are located (`"./data_DD/"`), then `"FLX_"`, and then the site name (the first three entries of the table containing data from all sites are for the site `"BE-Bra"`), and then some more specifications, inluding the years that respective files' data cover. What's the most effective way to extract the site name from all these strings? The stringr R package offers a set of very handy tools to work with strings. Here, we would like to extract the six characters, starting at position 15, and overwrite the values of colulmn `"siteid"` with just the six characters of the site name.
```{r}
library(stringr)
vec_sites <- str_sub(vec_files, start = 15, end = 20)
head(vec_sites)
```

And applied to the respective column of our dataframe:
```{r}
df_allsites <- df_allsites %>% 
  mutate(siteid = str_sub(siteid, start = 15, end = 20))

df_allsites
```

#### Functional programming II

- map
- nest
- mutate combined with purrr

Functions can be applied to any list. Above, the list we used the vector or file paths and converted it into a list and then applied a function (`read_csv()`) to that list using `purrr::map()`. The output was again a list (`list_df`). Because lists can consist of any types of objects, this is a powerful approach to "iterating" over list elements that can be used for all sorts of tasks.  The following takes the returned list of data frames containing daily data, and fits a linear regression model of GPP versus the incoming photosynthetic photon flux density (PPFD) to each sites' daily data.
```{r}
list_linmod <- try( map(list_df, ~lm(GPP_NT_VUT_REF ~ PPFD_IN, data = .)) )
```

Some sites have no non-NA data for PPFD_IN or GPP_NT_VUT_REF. 
```{r}
get_n_ppfd <- function(df){sum(!is.na(df$PPFD_IN))}
get_n_gpp  <- function(df){sum(!is.na(df$GPP_NT_VUT_REF))}

list_n_ppfd <- map_int(list_df, ~get_n_ppfd(.))    # map_int returns not a list, but automatically a vector or integers
idx_zero_ppfd <- which(list_n_ppfd == 0)

list_n_gpp <- map_int(list_df, ~get_n_gpp(.))    # map_int returns not a list, but automatically a vector or integers
idx_zero_gpp <- which(list_n_gpp == 0)
```

The following files have no PPFD data:
```{r}
names(list_df)[idx_zero_ppfd]
```

The following files have no GPP data:
```{r}
names(list_df)[idx_zero_gpp]
```

Let's drop them for all further analyses.
```{r}
list_df <- list_df[-c(idx_zero_ppfd, idx_zero_gpp)]
vec_sites <- vec_sites[-c(idx_zero_ppfd, idx_zero_gpp)]
df_allsites <- df_allsites %>% 
  filter(siteid %in% vec_sites)
```

And attempt to get the linear models by sites again.
```{r}
list_linmod <- map(list_df, ~lm(GPP_NT_VUT_REF ~ PPFD_IN, data = .))
```
Ok.

This returns a list of linear model objects (returned objects of the `lm()` function call). 

We can spin this further and apply (or map) the `summary()` function to the lm objects to get a list of useful statistics and metrics, and then further extract the element `r.squared"` from that list as:
```{r}
list_linmod %>% 
  map(summary) %>%              # applyting a function
  map_dbl("r.squared") %>%      # extracting from a named list
  head() # for handy output
```

When writing code for an analysis, it's useful, if not essential, to understand the objects we're working with and make sense of the results of simple `print <object>` statements. Data frames are particularly handy as they provide an organisation of data that is particularly intuitive and follows the tidy paradigm (variables along columns, observations along rows, values in cells). We've encountered such data frames above. Here, we're dealing with a list of linear model objects. Can such a list fit into the tidy paradigm? 

Yes, they can. Think of the linear model objects as 'values'. Values don't necessarily have to be scalars, but they can be of any type (class).
```{r}
tibble(
  siteid = vec_sites, 
  linmod = list_linmod
  )
```

The fact that nested cells can contain any type of object offers a powerful concept. Instead of a linear model object as in the example above, each cell may also contain another data frame. We say that the data frame is no longer flat, but nested.

The following creates a nested dataframe, where the column `data` is defined by the list of dataframes read from files above (`list_df`).
```{r}
tibble(
  siteid = vec_sites, 
  data = list_df
  )
```

We can achieve the same result, by directly "nesting" the flat dataframe holding all sites' data. This is done by combining the `group_by()`, which we have encountered above when aggregating using `summarise()`, with the function `nest()` from the tidyr package.
```{r}
library(tidyr)
df_allsites %>% 
  group_by(siteid) %>% 
  nest()
```
The function `nest()` names the nested data column automatically  `"data"`.

This structure is very useful. For example for applying functions over sites' dataframes individually (and not over the entire dataframe). By combining `map()` and `mutate()`, we can fit linear models on each site's dataframe individually in one go.
```{r}
df_allsites %>% 
  group_by(siteid) %>% 
  nest() %>% 
  mutate(linmod = map(data, ~lm(GPP_NT_VUT_REF ~ PPFD_IN, data = .)))
```

Again, this can be spun further, with the same steps as done above, to: 
```{r}
df_allsites <- df_allsites %>% 
  group_by(siteid) %>% 
  nest() %>% 
  mutate(linmod = map(data, ~lm(GPP_NT_VUT_REF ~ PPFD_IN, data = .))) %>% 
  mutate(summ = map(linmod, ~summary(.))) %>% 
  mutate(rsq = map_dbl(summ, "r.squared")) %>% 
  
  ## to arrange output, with highest r-squared on top
  arrange(desc(rsq))
```

#### Combining

- join
- Relational data

[link](https://r4ds.had.co.nz/relational-data.html#mutating-joins)

Let's get meta data for sites from [Falge et al.](https://daac.ornl.gov/FLUXNET/guides/Fluxnet_site_DB.html). The file `"fluxnet_site_info_all.csv"` is made available in directory `"data_DD"`.
```{r}
df_sites <- read_csv("./data_DD/fluxnet_site_info_all.csv")
df_sites
```

This contains info for many more sites (844 rows for 844 sites) than we have data for (36). On the other hand, we have data for multiple time steps for each of our 36 sites. Although these two tables contain different types of information (site-level vs. temporal), the two are related. The key variable that combines the two is the standard site ID that is commonly used for FLUXNET sites. In the sites table (`df_sites`), the key is called `fluxnetid`. In the temporal dataset `df_allsites`, we have created the site key column `siteid` ourselves. 

Now, we would like to combine (or merge, or join) the two tables, To do this, we have to identify a key variable, present in both datasets, and by which the two are combined. This requires us to rename it in one.
```{r}
df_allsites <- df_sites %>% 
  select(-siteid) %>%   # remove this variable because it doesn't contain the name we want
  rename(siteid = fluxnetid) %>% 
  right_join(df_allsites,
             by = "siteid") %>% 
  
  ## perform some variable renaming for our own taste
  rename(lon = longitude,
         lat = latitude,
         elv = gtopo30_elevation
  )
```


#### Advanced data vis

- facet_wrap
- factors (boxplot)
- histogram, density
- alpha, density, ... [link](https://ggplot2-book.org/statistical-summaries.html#overplotting)

gpp vs. ppfd by site (facet wrap)
Needs flat dataframe again.

```{r}
df_allsites %>% 
  unnest(data) %>% 
  ggplot(aes(x = PPFD_IN, y = GPP_NT_VUT_REF)) +
  geom_point(alpha = 0.1) +
  facet_wrap(~siteid)
```

histogram, density of daily gpp
```{r}
df_allsites %>% 
  unnest(data) %>% 
  ggplot(aes(x = GPP_NT_VUT_REF, y = ..density..)) +
  geom_histogram() +
  geom_density(color = "red")
```

factor - boxplot (gpp by vegetation type)
```{r}
df_allsites %>% 
  unnest(data) %>% 
  ggplot(aes(y = GPP_NT_VUT_REF, x = igbp_land_use)) +
  geom_boxplot() +
  coord_flip()
```

rsq by evergreen or not
```{r}
df_allsites %>% 
  mutate(evergreen = stringr::str_detect(lai_fpar, "Evergreen")) %>% 
  ggplot(aes(y = rsq, x = evergreen)) +
  geom_boxplot()
```

#### Tidy data

Download data from Groenigen et al., 2014:



#### Wide and long table formats: pivont_longer, ...

[link](https://r-bootcamp.netlify.app/chapter4)
[link](https://r4ds.had.co.nz/tidy-data.html)

#### Factors, dimensions

[link](https://datacarpentry.org/R-ecology-lesson/04-visualization-ggplot2.html)

#### Flat and nested data frames: tidyr



## Assignment

- Using dataset 1 (HH), plot the mean diurnal cycle of GPP and ET by season (DJF, MAM, JJA, SON). Clean data beforehand to remove XXX.
- Using dataset 1 (HH), visualise the light response (GPP vs. PPFD) during the growing season (within 80% of the overall maximum GPP) at half-hourly, daily, weekly and monthly time scales. Use only daytime data.
- Plot mean growing-season GPP vs. PAR across different sites, colors indicating vegetation type (or aridity)

